# Methodology {#methodology}

## Dynamic factor model

Following @stocwats:2016, the DFM represents the evolution of a vector of $N$ observed time series, $X_{t}$, in terms of a reduced number of unobserved common factors which evolve over time, plus uncorrelated disturbances which represent measurement error and/or idiosyncratic dynamics of the individual series. Throughout this chpater, we use lag operator, so that $a(L) = \sum_{i = 0}^{\infty} a_{i}L^{i}$, where $L$ is the lag operator, and $a(L)X_{t} = \sum_{i = 0}^{\infty} a_{i}X_{t - i}$.

One form of write the model is its dynamic form which represents the dependence of $X_{t}$ on lags of the factors explicitly.

### Dynamic form of the DFM

The DFM expresses a $N \times 1$ vector $X_{t}$ of observed time series variables as depending on a reduced number $q$ of unobserved or latent factors $f_{t}$ and a mean-zero idiosyncratic component $e_{t}$. The DFM is,

\begin{eqnarray}
  X_{t} & = & \lambda (L) f_{t} + e_{t} (\#eq:common)
  \\
  f_{t} & = & \Psi (L) f_{t} + \eta_{t} (\#eq:evolve)
\end{eqnarray}

where tha lag polynomial matrices $\lambda(L)$ and $\Psi(L)$ are $N \times q$ and $q \times q$, respectively, and $\eta_{t}$ is the $q \times 1$ vector of (serially uncorrelated) mean-zero innovations to the factors. The idiosyncratic disturbances are assumed to be uncorrelated with the factor innovations at all leads and lags, that is, $\E(e_{t}\eta_{t - k}') = 0$ for all $k$. The $i$th row of $\lambda(L)$, the lag polynomial $\lambda_{i}(L)$, is called the dynamic factor loading for the $i$th series, $X_{it}$. The term $\lambda_{i}(L)f_{t}$ in \@ref(eq:common) is the *common component* of the $i$th series.

If the idiosyncratic disturbance $e_{t}$ in \@ref(eq:common) is serially correlated the models \@ref(eq:common) and \@ref(eq:evolve) are incompletely specified. @chamroth:1983 *aproximate factor model* allows for such correlation.

### Static (stacked) form of the DFM

The *static*, or *stacked*, form of the DFM rewrites the dynamic form \@ref(eq:common) and \@ref(eq:evolve) to depend on $r$ *static factors* $F_{t}$ instead of the $q$ dynamic factors $f_{t}$, where $r \geq q$. This rewriting makes the model amenable to principal component analysis.

Let $p$ be the degree of the lag polynomial matrix $\lambda(L)$ and let $F_{t} = (f_{t}', f_{t - 1}', \ldots, f_{t - p}')'$ denote an $r \times 1$ vector of so-called "static" factors ---in contrast to the "dynamic" factors $f_{t}$. Also let $\Lambda = c(\lambda_{0}, \lambda_{1}, \ldots, \lambda_{p})$, where $\lambda_{h}$ is the $N \times q$ matrix of coefficients on the $h$th lag in $\lambda(L)$. Similarly, let $\Phi(L)$ be the matrix consisting of $1$s, $0$s, and the elements of $\Psi(L)$ such that the vector autoregression in \@ref(eq:common) is rewritten in terms of $F_{t}$. With this notation the DFM \@ref(eq:common) and \@ref(eq:evolve) can be rewritten, 

\begin{eqnarray}
  X_{t} & = & \Lambda F_{t} + e_{t} (\#eq:common-stack)
  \\
  F_{t} & = & \Phi(L) F_{t - 1} + G \eta_{t} (\#eq:evolve-stack) 
\end{eqnarray}

where $G = [I_{q} \quad 0_{q \times (r -q)}]'$.

### Normalization of the factors

Because the factors are unobserved, they are identified only up to arbitrary normalizations. In the principal components normalization ---@baing:2013 refer to this normalization as the PC1 normalization--- the columns of $\Lambda$ are orthogonal and are scaled to have unit norm:

\begin{equation}
  N^{-1} \Lambda' \Lambda = I_{r} (\#eq:normalization)
\end{equation}

where $\sum_{F} = \E(F_{t}F_{t}')$ is diagonal.

### Estimation of the Factors and DFM Parameters

The parameters and factors of the DFM can be estimated using nonparametric methods related to principal components analysis or by parametric state-space methods.

#### Nonparametric Methods and Principal Components Estimation

Nonparametric methods estimate the static factors in \@ref(eq:common-stack) directly without specifying a model for the factors or assuming specific distributions for the disturbances. These approaches use cross-sectional averaging to remove the influence of the idiosyncratic disturbances, leaving only the variation associated with the factors.

Principal components solve the least-squares problem in which $\Lambda$ and $F_{t}$ in \@ref(eq:common-stack) are treated as unknown parameters to be estimated:

\begin{equation}
  \min_{F_{1}, \ldots, F_{T}, \Lambda} V_{r}(\Lambda, F), \text{where } V_{r}(\Lambda, F) = \frac{1}{NT} \sum_{t = 1}^{T} (X_{t} - \Lambda F_{t})'(X_{t} - \Lambda F_{t}) (\#eq:pc)
\end{equation}

subject to the normalization \@ref(eq:normalization). If there are no missing data, then the solution to the least-squares problem in \@ref(eq:pc) is the PC estimator of the factors, $\hat{F}_{t} = N^{-1} \hat{\Lambda}' X_{t}$, where $\hat{\Lambda}$ is the matrix of eigenvectors of the sample variance matrix of $X_{t}$, $\hat{\sum}_{X} = T^{-1} \sum_{t = 1}^{T} X_{t}X_{t}'$, associated with the $r$ largest eigenvalues of $\hat{\sum}_{X}$.

#### Parametric state-space methods

State-space estimation entails specifying a full parametric model for $X_{t}$, $e_{t}$, and $f_{t}$ in the dynamic form of the DFM, so that the likelihood can be computed.

For parametric estimation, additional assumptions need to be made on the distribution of the errors and the dynamics of the idiosyncratic component $e_{t}$ in the DFM. A simple and tractable model is to suppose that the $i$th idiosyncratic disturbance, $e_{t}$, follows the univariate autoregression, 

\begin{equation}
  e_{it} = \delta_{i} (L) e_{it - 1} + \nu_{it} (\#eq:ar)
\end{equation}

where $\nu_{it}$ is serially uncorrelated. With the further assumptions that the disturbances $\nu_{it}$ in \@ref(eq:ar) are i.i.d. $\mathcal{N}(o, \sigma_{\nu_{i}}^{2}$), $i = 1, \ldots, N$, $\eta_{t}$ is i.i.d. $\mathcal{N}(0, \sum_{\eta})$, and $\{\nu_{t}\}$ and $\{\eta_{t}\}$ are independent, equations \@ref(eq:common), \@ref(eq:evolve) and \@ref(eq:ar) constitute a complete linear state-space model.

Given the parameters, the Kalman filter can be used to compute the likelihood and the Kalman smoother can be used to compute estimates of $f_{t}$ given the full-sample data on $\{X_{t}\}$. The likelihood can be maximized to obtain maximum likelihood estimates of the parameters. The fact that the state-space approach uses intertemporal smoothing to estimate the factors, whereas principal components approachs use only contemporaneous smoothing (averaging across series at the same date) is an important difference between the methods.

#### Hybrid methods

One way to handle the computational problem of maximum likelihood estimation of the state-space parameters is to adopt a two-step hybrid approach that combines the speed of principal components and the efficiency of the Kalman filter [@dozgianreic:2011]. In the first step, initial estimates of factors are obtained using principal components, from which the factor loadings are estimated and a model is fit to the idiosyncratic components. In the second step, the resulting parameters are used to construct a state-space model which then can be used to estimate $F_{t}$ by the Kalman filter.
