# Methodology {#methodology}

## Dynamic factor model

Following @stocwats:2016, the DFM represents the evolution of a vector of $N$ observed time series, $X_{t}$, in terms of a reduced number of unobserved common factors which evolve over time, plus uncorrelated disturbances which represent measurement error and/or idiosyncratic dynamics of the individual series. Throughout this chpater, we use lag operator, so that $a(L) = \sum_{i = 0}^{\infty} a_{i}L^{i}$, where $L$ is the lag operator, and $a(L)X_{t} = \sum_{i = 0}^{\infty} a_{i}X_{t - i}$.

One form of write the model is its dynamic form which represents the dependence of $X_{t}$ on lags of the factors explicitly.

### Dynamic form of the DFM

The DFM expresses a $N \times 1$ vector $X_{t}$ of observed time series variables as depending on a reduced number $q$ of unobserved or latent factors $f_{t}$ and a mean-zero idiosyncratic component $e_{t}$. The DFM is,

\begin{eqnarray}
  X_{t} & = & \lambda (L) f_{t} + e_{t} (\#eq:common)
  \\
  f_{t} & = & \Psi (L) f_{t} + \eta_{t} (\#eq:evolve)
\end{eqnarray}

where tha lag polynomial matrices $\lambda(L)$ and $\Psi(L)$ are $N \times q$ and $q \times q$, respectively, and $\eta_{t}$ is the $q \times 1$ vector of (serially uncorrelated) mean-zero innovations to the factors. The idiosyncratic disturbances are assumed to be uncorrelated with the factor innovations at all leads and lags, that is, $\mathbb{E}(e_{t}\eta_{t - k}') = 0$ for all $k$. The $i$th row of $\lambda(L)$, the lag polynomial $\lambda_{i}(L)$, is called the dynamic factor loading for the $i$th series, $X_{it}$. The term $\lambda_{i}(L)f_{t}$ in \@ref(eq:common) is the *common component* of the $i$th series.

If the idiosyncratic disturbance $e_{t}$ in \@ref(eq:common) is serially correlated the models \@ref(eq:common) and \@ref(eq:evolve) are incompletely specified. @chamroth:1983 *aproximate factor model* allows for such correlation.

### Static (stacked) form of the DFM

The *static*, or *stacked*, form of the DFM rewrites the dynamic form \@ref(eq:common) and \@ref(eq:evolve) to depend on $r$ *static factors* $F_{t}$ instead of the $q$ dynamic factors $f_{t}$, where $r \geq q$. This rewriting makes the model amenable to principal component analysis.

Let $p$ be the degree of the lag polynomial matrix $\lambda(L)$ and let $F_{t} = (f_{t}', f_{t - 1}', \ldots, f_{t - p}')'$ denote an $r \times 1$ vector of so-called "static" factors ---in contrast to the "dynamic" factors $f_{t}$. Also let $\Lambda = c(\lambda_{0}, \lambda_{1}, \ldots, \lambda_{p})$, where $\lambda_{h}$ is the $N \times q$ matrix of coefficients on the $h$th lag in $\lambda(L)$. Similarly, let $\Phi(L)$ be the matrix consisting of $1$s, $0$s, and the elements of $\Psi(L)$ such that the vector autoregression in \@ref(eq:common) is rewritten in terms of $F_{t}$. With this notation the DFM \@ref(eq:common) and \@ref(eq:evolve) can be rewritten, 

\begin{eqnarray}
  X_{t} & = & \Lambda F_{t} + e_{t} (\#eq:common-stack)
  \\
  F_{t} & = & \Phi(L) F_{t - 1} + G \eta_{t} (\#eq:evolve-stack) 
\end{eqnarray}

where $G = [I_{q} \quad 0_{q \times (r -q)}]'$.

### Normalization of the factors

Because the factors are unobserved, they are identified only up to arbitrary normalizations. In the principal components normalization ---@baing:2013 refer to this normalization as the PC1 normalization--- the columns of $\Lambda$ are orthogonal and are scaled to have unit norm:

\begin{equation}
  N^{-1} \Lambda' \Lambda = I_{r} (\#eq:normalization)
\end{equation}

where $\sum_{F} = \mathbb{E}(F_{t}F_{t}')$ is diagonal.

### Estimation of the Factors and DFM Parameters

The parameters and factors of the DFM can be estimated using nonparametric methods related to principal components analysis or by parametric state-space methods.

#### Nonparametric Methods and Principal Components Estimation

Nonparametric methods estimate the static factors in \@ref(eq:common-stack) directly without specifying a model for the factors or assuming specific distributions for the disturbances. These approaches use cross-sectional averaging to remove the influence of the idiosyncratic disturbances, leaving only the variation associated with the factors.

Principal components solve the least-squares problem in which $\Lambda$ and $F_{t}$ in \@ref(eq:common-stack) are treated as unknown parameters to be estimated:

\begin{equation}
  \min_{F_{1}, \ldots, F_{T}, \Lambda} V_{r}(\Lambda, F), \text{where } V_{r}(\Lambda, F) = \frac{1}{NT} \sum_{t = 1}^{T} (X_{t} - \Lambda F_{t})'(X_{t} - \Lambda F_{t}) (\#eq:pc)
\end{equation}

subject to the normalization \@ref(eq:normalization). If there are no missing data, then the solution to the least-squares problem in \@ref(eq:pc) is the PC estimator of the factors, $\hat{F}_{t} = N^{-1} \hat{\Lambda}' X_{t}$, where $\hat{\Lambda}$ is the matrix of eigenvectors of the sample variance matrix of $X_{t}$, $\hat{\sum}_{X} = T^{-1} \sum_{t = 1}^{T} X_{t}X_{t}'$, associated with the $r$ largest eigenvalues of $\hat{\sum}_{X}$.

#### Parametric state-space methods

State-space estimation entails specifying a full parametric model for $X_{t}$, $e_{t}$, and $f_{t}$ in the dynamic form of the DFM, so that the likelihood can be computed.

For parametric estimation, additional assumptions need to be made on the distribution of the errors and the dynamics of the idiosyncratic component $e_{t}$ in the DFM. A simple and tractable model is to suppose that the $i$th idiosyncratic disturbance, $e_{t}$, follows the univariate autoregression, 

\begin{equation}
  e_{it} = \delta_{i} (L) e_{it - 1} + \nu_{it} (\#eq:ar)
\end{equation}

where $\nu_{it}$ is serially uncorrelated. With the further assumptions that the disturbances $\nu_{it}$ in \@ref(eq:ar) are i.i.d. $\mathcal{N}(o, \sigma_{\nu_{i}}^{2}$), $i = 1, \ldots, N$, $\eta_{t}$ is i.i.d. $\mathcal{N}(0, \sum_{\eta})$, and $\{\nu_{t}\}$ and $\{\eta_{t}\}$ are independent, equations \@ref(eq:common), \@ref(eq:evolve) and \@ref(eq:ar) constitute a complete linear state-space model.

Given the parameters, the Kalman filter can be used to compute the likelihood and the Kalman smoother can be used to compute estimates of $f_{t}$ given the full-sample data on $\{X_{t}\}$. The likelihood can be maximized to obtain maximum likelihood estimates of the parameters. The fact that the state-space approach uses intertemporal smoothing to estimate the factors, whereas principal components approachs use only contemporaneous smoothing (averaging across series at the same date) is an important difference between the methods.

#### Hybrid methods

One way to handle the computational problem of maximum likelihood estimation of the state-space parameters is to adopt a two-step hybrid approach that combines the speed of principal components and the efficiency of the Kalman filter [@dozgianreic:2011]. In the first step, initial estimates of factors are obtained using principal components, from which the factor loadings are estimated and a model is fit to the idiosyncratic components. In the second step, the resulting parameters are used to construct a state-space model which then can be used to estimate $F_{t}$ by the Kalman filter.

## VAR {#var}

### Lag order {#lag-order}

It is necessary to choose the lag-order $p$ of the VAR models calculated in the next sections. First, defining the maximum lag order that is considered reasonable as $p_{max}$ and the minimum lag order $p_{min}$ set to zero one criterion is whether the lag-order estimator is consistent for the true lag-order $p_{0}$ where the DGP is VAR($p_{0}$), provided $p_{min} \leq p_{0} \leq p_{max}$. The Schwarz Information Criterion (SIC) and the Hannan-Quin Criterion (HQC) are consistent for $p_{0}$ in contrast to the Akaike Information Criterion (AIC) [@lutkepohl:2005]. 

On the other hand, it is worthly examine the finite sample properties of the lag-order estimators. Even if we grant the premise that $p_{min} \leq p_{0} \leq p_{max}$, one may not want to overrate the importance of the lag-order estimator being consistent. The convergence of $\hat{p}$ toward $p_{0}$ can be very slow in practice, and in small samples consistent lag-order selection criteria tend to be strongly downbiased toward $p_{min}$ [@kilian:2001]. In small samples the distribution of the AIC lag-order estimates tends to be more balanced about the true lag order than for the remaining criteria [@kililutk:2017]. Likewise, because this study have no interest in the lag order of the process but impulse responses, forecast, and related statistics that can be written as smooth functions of VAR model parameters it is necessary assure that these statistics of interest can be consistenly estimated. They are consistently estimated as long as the lag order is not underestimated asymptotically. Besides, the probability of the AIC overfitting the VAR model is negligible asymptotically [@paultjos:1985]. So efficiency lost is not a major concern.

Furthermore, the MSE of the impulse-response estimates is lower for models estimated with lags selected by the AIC in contrast with more parsimonious criteria as SIC and HQC [@ventlutz:2005]. 

As a result, the relevant criteria used in this study is the AIC as it is the most reliable estimator of $p_{0}$ compared with other information criteria [@gonzpita:2002].
