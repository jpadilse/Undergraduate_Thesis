# Methodology {#methodology}

For the construction of the non-asset-market uncertainty index is necessary capture the common comovements in the series for which is used a dynamic factor model that is described in [Dynamic factor model]. 

## Dynamic Factor Model {#dynamic-factor-model}

Following @stocwats:2016, the DFM represents the evolution of a vector of $N$ observed time series, $X_{t}$, in terms of a reduced number of unobserved common factors which evolve over time, plus uncorrelated disturbances which represent measurement error and/or idiosyncratic dynamics of the individual series. Throughout this chapter, we use lag operator, so that $a(L) = \sum_{i = 0}^{\infty} a_{i}L^{i}$, where $L$ is the lag operator, and $a(L)X_{t} = \sum_{i = 0}^{\infty} a_{i}X_{t - i}$.

One form of write the model is its dynamic form which take into account the serial dependence in the data, i.e., represents the dependence of $X_{t}$ on lags of the factors explicitly.

### Dynamic Form of the DFM

The DFM expresses a $N \times 1$ vector $X_{t}$ of observed time series variables as depending on a reduced number $q$ of unobserved or latent factors $f_{t}$ and a mean-zero idiosyncratic component $e_{t}$. The DFM is,

\begin{eqnarray}
  X_{t} & = & \lambda (L) f_{t} + e_{t} (\#eq:common)
  \\
  f_{t} & = & \Psi (L) f_{t} + \eta_{t} (\#eq:evolve)
\end{eqnarray}

where $X_{t} \stackrel{iid}{\sim} (0, \sum_{x})$ and the lag polynomial matrices $\lambda(L)$ and $\Psi(L)$ are $N \times q$ and $q \times q$, respectively. Also, $\eta_{t}$ is the $q \times 1$ vector of (serially uncorrelated) mean-zero innovations to the factors such that the factor innovations and the idiosyncratic disturbances are assumed to be orthogonal , i.e., $\mathbb{E}(e_{t}\eta_{s}') = 0 ~ \forall ~ t, s$. The $i$th row of $\lambda(L)$, the lag polynomial $\lambda_{i}(L)$, is called the dynamic factor loading for the $i$th series, $X_{it}$. The term $\lambda_{i}(L)f_{t}$ in \@ref(eq:common) is the *common component* of the $i$th series. Because in the basic model \@ref(eq:common) the observed variables are assumed to have mean zero the uncertainty indices in [Data] are transformed prior to the analysis.

If the idiosyncratic disturbance $e_{t}$ in \@ref(eq:common) is serially correlated the models \@ref(eq:common) and \@ref(eq:evolve) are incompletely specified. @chamroth:1983 *approximate factor model* allows for such correlation.

### Static (Stacked) Form of the DFM

The *static*, or *stacked*, form of the DFM rewrites the dynamic form \@ref(eq:common) and \@ref(eq:evolve) to depend on $r$ *static factors* $F_{t}$ instead of the $q$ dynamic factors $f_{t}$, where $r \geq q$. This rewriting makes the model amenable to principal component analysis.

Let $p$ be the degree of the lag polynomial matrix $\lambda(L)$ and let $F_{t} = (f_{t}', f_{t - 1}', \ldots, f_{t - p}')'$ denote an $r \times 1$ vector of *static* factors ---in contrast to the shorter vector of *dynamic* factors $f_{t}$. Also let $\Lambda = c(\lambda_{0}, \lambda_{1}, \ldots, \lambda_{p})$, where $\lambda_{h}$ is the $N \times q$ matrix of coefficients on the $h$th lag in $\lambda(L)$. Similarly, let $\Phi(L)$ be the matrix consisting of $1$s, $0$s, and the elements of $\Psi(L)$ such that the vector autoregression in \@ref(eq:common) is rewritten in terms of $F_{t}$. With this notation the DFM \@ref(eq:common) and \@ref(eq:evolve) can be rewritten, 

\begin{eqnarray}
  X_{t} & = & \Lambda F_{t} + e_{t} (\#eq:common-stack)
  \\
  F_{t} & = & \Phi(L) F_{t - 1} + G \eta_{t} (\#eq:evolve-stack) 
\end{eqnarray}

where 

\begin{equation*}
  \Phi = 
  \begin{bmatrix}
    \Psi_{1} & \Psi_{2} & \ldots & \Psi_{p} & \Psi_{p + 1} \\
    I_{q} & 0 & \ldots & 0 & 0 \\
    0 & I_{q} & \ldots & 0 & 0 \\
    \vdots & \vdots & \ddots & \vdots & \vdots \\
    0 & 0 & \ldots & I_{q} & 0
  \end{bmatrix}_{r \times r} \qquad
  \text{and} \qquad
  G = 
  \begin{bmatrix}
    I_{q} \\
    0
  \end{bmatrix}_{r \times q}
\end{equation*}

### Normalization of the Factors

Because the factors are unobserved, they are identified only up to arbitrary normalizations. Exact identification of the factor model can be ensured with the principal components normalization ---@baing:2013 refer to this normalization as the PC1 normalization--- where the columns of $\Lambda$ are orthonormal:

\begin{equation}
  N^{-1} \Lambda' \Lambda = I_{r} (\#eq:normalization)
\end{equation}

and choosing the factors such that $\sum_{F} = \mathbb{E}(F_{t}F_{t}')$ is a diagonal matrix with distinct diagonal elements in decreasing order. In other words, the first factor has the largest variance and, hence, explains the largest part of the variance of $X_{t}$ among the common factors. The second factor, has the second largest variance, and so on. These conditions are sufficient to guarantee identification of $\Lambda$ but only up to the sign of its columns.

### Estimation of the Factors and DFM Parameters

The parameters and factors of the DFM can be estimated using nonparametric methods related to principal components analysis or by parametric state-space methods.

#### Nonparametric Methods and Principal Components Estimation

Nonparametric methods estimate the static factors in \@ref(eq:common-stack) directly without specifying a model for the factors or assuming specific distributions for the disturbances. These approaches use cross-sectional averaging to remove the influence of the idiosyncratic disturbances, leaving only the variation associated with the factors.

Principal components minimize the variance of the idiosyncratic components, i.e., maximizing the part of the variance of the observed variables explained by the common factors in which $\Lambda$ and $F_{t}$ in \@ref(eq:common-stack) are treated as unknown parameters to be estimated:

\begin{equation}
  \min_{F_{1}, \ldots, F_{T}, \Lambda} V_{r}(\Lambda, F), \text{where } V_{r}(\Lambda, F) = \frac{1}{NT} \sum_{t = 1}^{T} (X_{t} - \Lambda F_{t})'(X_{t} - \Lambda F_{t}) (\#eq:pc)
\end{equation}

subject to the normalization \@ref(eq:normalization). The solution to the least-squares problem in \@ref(eq:pc) is the principal components (PC) estimator of the factors, $\hat{F}_{t} = N^{-1} \hat{\Lambda}' X_{t}$, where $\hat{\Lambda}$ is the matrix of eigenvectors of the sample variance matrix of $X_{t}$, $\hat{\sum}_{X} = T^{-1} \sum_{t = 1}^{T} X_{t}X_{t}'$, associated with the $r$ largest eigenvalues of $\hat{\sum}_{X}$, and $\hat
{\sum}_{F} = T^{-1} \sum_{t = 1}^{T} \hat{F}_{t} \hat{F}_{t}' = \hat{\Lambda}' \hat{\sum}_{X} \hat{\Lambda} = \diag(\lambda_{1}, \ldots, \lambda_{r})$. The eigenvalues $\lambda_{1}, \ldots, \lambda_{r}$ are the empirical variances of the factors with $\lambda_{1}$ representing the variance of the principal component with the largest contribution to the variance of the data, $\lambda_{2}$ represents the variance of the second-most important component, and so on. The properties of this estimator are well discussed in @stocwats:2002.

#### Parametric state-space methods

State-space estimation entails specifying a full parametric model for $X_{t}$, $e_{t}$, and $f_{t}$ in the dynamic form of the DFM, so that the likelihood can be computed.

For parametric estimation, additional assumptions need to be made on the distribution of the errors and the dynamics of the idiosyncratic component $e_{t}$ in the DFM. A simple and tractable model is to suppose that the $i$th idiosyncratic disturbance, $e_{t}$, follows the univariate autoregression, 

\begin{equation}
  e_{it} = \delta_{i} (L) e_{it - 1} + \nu_{it} (\#eq:ar)
\end{equation}

where $\nu_{it}$ is serially uncorrelated. With the further assumptions that $\nu_{it} \stackrel{iid}{\sim} \mathcal{N}(o, \sigma_{\nu_{i}}^{2})$ , $i = 1, \ldots, N$, and $\eta_{t} \stackrel{iid}{\sim} \mathcal{N}(0, \sum_{\eta})$, and $\{\nu_{t}\}$ and $\{\eta_{t}\}$ are independent, equations \@ref(eq:common), \@ref(eq:evolve) and \@ref(eq:ar) constitute a complete linear state-space model.

Given the parameters, the Kalman filter can be used to compute the likelihood and the Kalman smoother can be used to compute estimates of $f_{t}$ given the full-sample data on $\{X_{t}\}$. The likelihood can be maximized to obtain maximum likelihood estimates of the parameters. The fact that the state-space approach uses intertemporal smoothing to estimate the factors, whereas principal components approach use only contemporaneous smoothing (averaging across series at the same date) is an important difference between the methods.

#### Hybrid methods

One way to handle the computational problem of maximum likelihood estimation of the state-space parameters is to adopt a two-step hybrid approach that combines the speed of principal components and the efficiency of the Kalman filter [@dozgianreic:2011]. In the first step, initial estimates of factors are obtained using principal components, from which the factor loadings are estimated and a model is fit to the idiosyncratic components. In the second step, the resulting parameters are used to construct a state-space model which then can be used to estimate $F_{t}$ by the Kalman filter.

## VAR {#var}

### Lag Order {#lag-order}

It is necessary to choose the lag-order $p$ of the VAR models calculated in the next sections. First, defining the maximum lag order that is considered reasonable as $p_{max}$ and the minimum lag order $p_{min}$ set to zero one criterion is whether the lag-order estimator is consistent for the true lag-order $p_{0}$ where the DGP is VAR($p_{0}$), provided $p_{min} \leq p_{0} \leq p_{max}$. The Schwarz Information Criterion (SIC) and the Hannan-Quin Criterion (HQC) are consistent for $p_{0}$ in contrast to the Akaike Information Criterion (AIC) [@lutkepohl:2005]. 

On the other hand, it is worthy examine the finite sample properties of the lag-order estimators. Even if we grant the premise that $p_{min} \leq p_{0} \leq p_{max}$, one may not want to overrate the importance of the lag-order estimator being consistent. The convergence of $\hat{p}$ toward $p_{0}$ can be very slow in practice, and in small samples consistent lag-order selection criteria tend to be strongly downbiased toward $p_{min}$ [@kilian:2001]. In small samples the distribution of the AIC lag-order estimates tends to be more balanced about the true lag order than for the remaining criteria [@kililutk:2017]. Likewise, because this study have no interest in the lag order of the process but impulse responses, forecast, and related statistics that can be written as smooth functions of VAR model parameters it is necessary assure that these statistics of interest can be consistently estimated. They are consistently estimated as long as the lag order is not underestimated asymptotically. Besides, the probability of the AIC overfitting the VAR model is negligible asymptotically [@paultjos:1985]. So efficiency lost is not a major concern.

Furthermore, the MSE of the impulse-response estimates is lower for models estimated with lags selected by the AIC in contrast with more parsimonious criteria as SIC and HQC [@ventlutz:2005]. 

<!--  as it is the most reliable estimator of $p_{0}$ compared with other information criteria [@gonzpita:2002]. -->

Although the AIC seems to be the best option for select the lag order, Leeb \& PÃ¶tscher [-@leebpots:2005;-@leebpots:2006] questioned the inference using data-dependent lag-order selection procedure for VAR models. One alternative approach that avoids this problems is use models with fixed lags instead. As a result, fixed lag orders for all VAR models are used. Also, since the data is monthly the appropriate lag order is 12 months.
